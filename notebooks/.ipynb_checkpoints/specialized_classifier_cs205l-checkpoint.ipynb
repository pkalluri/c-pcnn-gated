{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from termcolor import colored, cprint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "opts = {}\n",
    "\n",
    "# Data settings\n",
    "opts['num_train_samples'] = 50\n",
    "opts['num_test_samples'] = 100\n",
    "opts['vocab_size'] = 1000 # Number of tokens in vocabulary\n",
    "\n",
    "# Model and ML settings\n",
    "opts['hidden_layer_size']=50\n",
    "opts['learning_rate']=1e-4\n",
    "opts['epochs']=100\n",
    "opts['batch_size']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary. Useful for converting words to tokens and vice versa.\n",
    "# Adapted from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.n_words = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2id:\n",
    "            self.word2id[word] = self.n_words\n",
    "            self.id2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "vocab = Vocabulary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [(117, [0, 1]), (129, [0, 1]), (770, [0, 1])] ...\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset mapping x=some number to a set Y=[0,1]\n",
    "\n",
    "def create_number_to_bimodal_data(num_samples, vocab_size, vocab):\n",
    "    data = []  # Each row will contain (x, [y, y', ...])\n",
    "\n",
    "    # Create vocab\n",
    "    for i in range(vocab_size):\n",
    "        vocab.add_word(str(i)) # if not already there\n",
    "    \n",
    "    # Create data\n",
    "    special_token_1 = vocab.word2id[str(0)]\n",
    "    special_token_2 = vocab.word2id[str(1)]\n",
    "    for i in range(num_samples):\n",
    "        x = 0\n",
    "        x_token = vocab.word2id[str(x)]\n",
    "        data.append((x_token, [special_token_1, special_token_2]))    \n",
    "    return data\n",
    "\n",
    "train_data = create_number_to_bimodal_data(opts['num_train_samples'], opts['vocab_size'], vocab)\n",
    "test_data = create_number_to_bimodal_data(opts['num_test_samples'], opts['vocab_size'], vocab)\n",
    "print('data:',train_data[0:3],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sample: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "\n",
    "def mark_onehot(i, empty_onehot):\n",
    "    empty_onehot.zero_()\n",
    "    empty_onehot.scatter_(1,torch.tensor([[i]]),1) # e.g. [0,0,1,0]\n",
    "\n",
    "onehot = torch.FloatTensor(1, vocab.n_words)\n",
    "def prep_x(x):\n",
    "    mark_onehot(x, onehot) # e.g. [0,0,1,0]\n",
    "    return onehot\n",
    "\n",
    "def prep_y(y):\n",
    "    return torch.LongTensor([y]) # e.g. y=[[2,0]]\n",
    "\n",
    "print('processed sample:', prep_x(train_data[0][0]), prep_y(train_data[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and loss,or loss\n",
    "class AndLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AndLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        target_onehot = torch.FloatTensor(opts['batch_size'], vocab.n_words)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, target, 1)   \n",
    "        loss = torch.log1p(prediction) * target_onehot\n",
    "        loss = -1 * torch.sum(loss)/opts['batch_size']  #Average loss across samples\n",
    "        return loss\n",
    "\n",
    "class OrLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        target_onehot = torch.FloatTensor(opts['batch_size'], vocab.n_words)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, target, 1)   \n",
    "        loss = prediction * target_onehot\n",
    "        loss = -1 * torch.log1p(torch.sum(loss))  #Per sample loss\n",
    "#         print(loss)\n",
    "        loss = loss/opts['batch_size']  #Average loss across samples\n",
    "        return loss\n",
    "    \n",
    "class MaxLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        target_onehot = torch.FloatTensor(opts['batch_size'], vocab.n_words)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, target, 1)   \n",
    "        loss = prediction * target_onehot\n",
    "        loss = -1 * torch.log1p(torch.max(loss))  #Per sample loss\n",
    "        loss = loss/opts['batch_size']  #Average loss across samples\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "and_criterion = AndLoss()\n",
    "or_criterion = OrLoss()\n",
    "max_criterion = MaxLoss()\n",
    "\n",
    "def official_criterion(prediction, target):\n",
    "    squeezed_target = target.squeeze()\n",
    "    nll = torch.nn.NLLLoss()\n",
    "    return nll(torch.log1p(prediction), squeezed_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show they match when only 1 y\n",
    "# prediction = Variable(torch.rand(opts['batch_size'], vocab.n_words).float())\n",
    "# print('pred '+str(prediction))\n",
    "# target = Variable(torch.FloatTensor(opts['batch_size'],3).uniform_(0, vocab.n_words).long())\n",
    "# print('target '+str(target))\n",
    "\n",
    "# and_loss = and_criterion(prediction, target)\n",
    "# print('and loss '+str(and_loss))\n",
    "# or_loss = or_criterion(prediction, target)\n",
    "# print('or loss '+str(or_loss))\n",
    "# # official_loss = official_criterion(prediction,target)\n",
    "# # print('loss '+str(official_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple neural net\n",
    "def get_new_model():\n",
    "    model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(vocab.n_words, opts['hidden_layer_size']),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(opts['hidden_layer_size'], vocab.n_words),\n",
    "    torch.nn.Softmax()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define training\n",
    "def train(data, model, criterion, opts, verbose=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=opts['learning_rate'])\n",
    "    running_loss=0.0\n",
    "    for epoch in range(opts['epochs']):\n",
    "        for (x,y) in data:\n",
    "            x = prep_x(x)\n",
    "            y = prep_y(y)\n",
    "            # Forward pass: compute predicted y\n",
    "            y_pred = model(x) \n",
    "            loss = criterion(y_pred, y)\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            # For printing stats\n",
    "            running_loss += loss.item()\n",
    "        if verbose: print('[Epoch %d] loss: %.3f' % (epoch, running_loss))\n",
    "        running_loss = 0.0\n",
    "        \n",
    "# Define testing\n",
    "def test(x, model, vocab):\n",
    "    output = model(x)\n",
    "    prediction_index = torch.argmax(output)\n",
    "#     prediction = vocab.index2word[prediction_index.item()]\n",
    "    return int(prediction_index.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontColors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_criterion(criterion):\n",
    "    print(\"Training...\")\n",
    "    train(train_data, model, criterion, opts, verbose=False)\n",
    "\n",
    "\n",
    "    print(\"Performance on Train set\")\n",
    "    font_color = FontColors.ENDC\n",
    "    num_correct = 0\n",
    "    for (x,y) in train_data: # Train data\n",
    "        prediction = test(prep_x(x), model, vocab)\n",
    "        is_correct = int(prediction) in y\n",
    "        if is_correct:\n",
    "            num_correct = num_correct + 1\n",
    "            font_color = FontColors.OKGREEN\n",
    "        print(font_color, x, y, '-->', prediction, is_correct, FontColors.ENDC)\n",
    "        font_color = FontColors.ENDC\n",
    "    print(num_correct/opts['num_train_samples'])\n",
    "\n",
    "\n",
    "    print(\"Performance on Test set\")\n",
    "    font_color = FontColors.ENDC\n",
    "    num_correct = 0\n",
    "    predictions = []\n",
    "    for (x,y) in test_data: # Train data\n",
    "        prediction = test(prep_x(x), model, vocab)\n",
    "        predictions.append(vocab.id2word[prediction])\n",
    "        is_correct = int(prediction) in y\n",
    "        if is_correct:\n",
    "            num_correct = num_correct + 1\n",
    "            font_color = FontColors.OKGREEN\n",
    "        print(font_color, x, y, '-->', prediction, is_correct, FontColors.ENDC)\n",
    "        font_color = FontColors.ENDC\n",
    "    print(num_correct/opts['num_test_samples'])\n",
    "    \n",
    "    plt.hist(predictions)\n",
    "    plt.axis([0,1,0,opts['num_test_samples']])\n",
    "#     plt.xlabel('Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pkalluri/env/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Train set\n",
      "\u001b[92m 117 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 129 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 770 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 363 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 33 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 435 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 734 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 520 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 633 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 248 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 614 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 456 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 738 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 795 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 98 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 767 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 24 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 514 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 702 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 602 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 839 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 476 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 848 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 577 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 622 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 180 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 142 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 572 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 832 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 865 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 357 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 77 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 493 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 345 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 121 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 649 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 811 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 268 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 706 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 36 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 618 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 146 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 96 [0, 1] --> 0 True \u001b[0m\n",
      "1.0\n",
      "Performance on Test set\n",
      "\u001b[92m 977 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 74 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 821 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 316 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 227 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 561 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 647 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 992 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 636 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 566 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 594 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 688 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 153 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 845 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 451 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 621 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 929 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 392 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 993 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 587 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 59 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 460 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 966 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 220 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 487 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 454 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 214 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 390 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 64 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 499 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 497 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 306 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 758 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 936 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 797 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 206 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 644 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 625 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 485 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 169 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 612 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 652 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 256 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 827 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 112 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 374 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 269 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 182 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 694 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 604 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 829 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 964 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 455 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 288 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 94 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 40 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 983 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 999 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 915 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 642 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 165 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 346 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 379 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 855 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 174 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 971 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 826 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 972 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 783 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 389 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 474 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 323 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 859 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 746 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 381 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 266 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 208 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 567 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 513 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 907 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 307 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 380 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 187 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 309 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 803 [0, 1] --> 0 True \u001b[0m\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_criterion(and_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pkalluri/env/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Train set\n",
      "\u001b[92m 117 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 129 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 770 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 363 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 33 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 435 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 734 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 520 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 633 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 248 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 614 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 456 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 738 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 795 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 98 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 767 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 24 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 514 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 702 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 602 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 839 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 476 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 848 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 577 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 622 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 180 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 142 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 572 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 832 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 865 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 357 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 77 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 493 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 345 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 121 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 649 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 811 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 268 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 706 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 36 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 618 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 146 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 96 [0, 1] --> 1 True \u001b[0m\n",
      "1.0\n",
      "Performance on Test set\n",
      "\u001b[92m 977 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 74 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 821 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 316 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 227 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 561 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 647 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 992 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 636 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 566 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 594 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 688 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 153 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 845 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 451 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 621 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 929 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 392 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 993 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 587 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 59 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 460 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 966 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 220 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 487 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 454 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 214 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 390 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 64 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 499 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 497 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 306 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 758 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 936 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 797 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 206 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 644 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 625 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 485 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 169 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 612 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 652 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 256 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 827 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 112 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 374 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 269 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 182 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 694 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 604 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 829 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 964 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 455 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 288 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 94 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 40 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 983 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 999 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 915 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 642 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 165 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 346 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 379 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 855 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 174 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 971 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 826 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 972 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 783 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 389 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 474 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 323 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 859 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 746 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 381 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 266 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 208 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 567 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 513 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 907 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 307 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 380 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 187 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 309 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 803 [0, 1] --> 1 True \u001b[0m\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_criterion(or_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pkalluri/env/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Train set\n",
      "\u001b[92m 117 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 129 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 770 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 363 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 33 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 435 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 734 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 520 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 633 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 248 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 614 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 456 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 738 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 795 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 98 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 767 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 24 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 514 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 702 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 602 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 839 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 476 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 848 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 577 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 622 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 180 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 142 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 572 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 832 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 865 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 357 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 77 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 493 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 345 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 121 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 649 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 811 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 268 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 706 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 36 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 618 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 146 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 96 [0, 1] --> 0 True \u001b[0m\n",
      "1.0\n",
      "Performance on Test set\n",
      "\u001b[92m 977 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 74 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 821 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 316 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 227 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 561 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 647 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 992 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 636 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 566 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 594 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 688 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 153 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 845 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 451 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 621 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 929 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 392 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 993 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 587 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 59 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 460 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 966 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 220 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 487 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 454 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 214 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 390 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 64 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 499 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 497 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 306 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 758 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 936 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 797 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 206 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 644 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 625 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 485 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 169 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 612 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 652 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 256 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 827 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 112 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 374 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 269 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 182 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 694 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 604 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 829 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 964 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 455 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 288 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 94 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 40 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 983 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 999 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 915 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 642 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 165 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 346 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 379 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 855 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 174 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 971 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 826 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 972 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 783 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 389 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 474 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 323 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 859 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 746 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 381 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 266 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 208 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 567 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 513 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 907 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 307 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 380 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 187 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 309 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 803 [0, 1] --> 0 True \u001b[0m\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAACk9JREFUeJzt3G/IXvddx/HP16S1ZmL/KSEmxRUWlKLIZqiVgsiq0P3B9sEYLaJhFPJk6nSCqz7p0xbEbYIMwjqNMOZKHbSIKCV2iA8sS7fh+kdtqHRNSduJaxQLztavD3IcISRLcp375oYvrxeU6zrn+p3rfB+9ezi5r1PdHQDm+r6dHgCA7SX0AMMJPcBwQg8wnNADDCf0AMNdMvRV9bmqer2qnjln3w1V9URVvbC8Xr/sr6r6o6o6WVX/WFXv2c7hAbi0y7mi/9Mkd5637/4kx7v7YJLjy3aSvC/JweW/I0k+szVjArCpS4a+u/8uyb+ft/uuJMeW98eS3H3O/j/rs/4hyXVVtW+rhgXgyu3e8Li93X16ef9qkr3L+/1JXj5n3all3+mcp6qO5OxVf+qqa37mqhsPbDjK1vmp/dfu9AgAl+3pp5/+t+7+kUut2zT039XdXVVX/ByF7j6a5GiSfP++g73v8KfWjrLaiQc/sNMjAFy2qnrpctZt+lc3r/3/LZnl9fVl/ytJbjpn3YFlHwA7ZNPQP57k8PL+cJLHztn/a8tf39yW5Mw5t3gA2AGXvHVTVV9I8gtJfriqTiV5IMmDSR6pqvuSvJTkw8vyv0ry/iQnk7yZ5CPbMDMAV+CSoe/uey/y0R0XWNtJPrp2KAC2jl/GAgwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcEIPMJzQAwwn9ADDCT3AcKtCX1W/XVXPVtUzVfWFqrqmqm6uqqeq6mRVfbGqrt6qYQG4chuHvqr2J/nNJIe6+yeT7EpyT5KHknyyu9+V5NtJ7tuKQQHYzNpbN7uT/EBV7U6yJ8npJO9N8ujy+bEkd688BwArbBz67n4lyR8k+WbOBv5MkqeTvNHdby3LTiXZf6Hjq+pIVZ2oqhNvv3lm0zEAuIQ1t26uT3JXkpuT/GiSdyS583KP7+6j3X2ouw/t2nPtpmMAcAlrbt38YpJ/7e5vdff/JPlSktuTXLfcykmSA0leWTkjACusCf03k9xWVXuqqpLckeS5JE8m+dCy5nCSx9aNCMAaa+7RP5Wz/+j61STfWL7raJJPJPl4VZ1McmOSh7dgTgA2tPvSSy6uux9I8sB5u19Mcuua7wVg6/hlLMBwQg8wnNADDCf0AMMJPcBwQg8wnNADDCf0AMMJPcBwQg8wnNADDCf0AMMJPcBwQg8wnNADDCf0AMMJPcBwQg8wnNADDCf0AMMJPcBwQg8wnNADDCf0AMMJPcBwQg8wnNADDCf0AMMJPcBwQg8wnNADDCf0AMMJPcBwQg8w3KrQV9V1VfVoVf1TVT1fVT9XVTdU1RNV9cLyev1WDQvAlVt7Rf/pJH/d3T+R5KeTPJ/k/iTHu/tgkuPLNgA7ZOPQV9W1SX4+ycNJ0t3f6e43ktyV5Niy7FiSu9cOCcDm1lzR35zkW0n+pKq+VlWfrap3JNnb3aeXNa8m2Xuhg6vqSFWdqKoTb795ZsUYAHwva0K/O8l7knymu9+d5L9y3m2a7u4kfaGDu/todx/q7kO79ly7YgwAvpc1oT+V5FR3P7VsP5qz4X+tqvYlyfL6+roRAVhj49B396tJXq6qH1923ZHkuSSPJzm87Duc5LFVEwKwyu6Vx/9Gks9X1dVJXkzykZz9n8cjVXVfkpeSfHjlOQBYYVXou/vrSQ5d4KM71nwvAFvHL2MBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmA4oQcYTugBhhN6gOGEHmC41aGvql1V9bWq+stl++aqeqqqTlbVF6vq6vVjArCprbii/1iS58/ZfijJJ7v7XUm+neS+LTgHABtaFfqqOpDkA0k+u2xXkvcmeXRZcizJ3WvOAcA6a6/oP5Xkd5P877J9Y5I3uvutZftUkv0XOrCqjlTViao68fabZ1aOAcDFbBz6qvpgkte7++lNju/uo919qLsP7dpz7aZjAHAJu1cce3uSX66q9ye5JskPJfl0kuuqavdyVX8gySvrxwRgUxtf0Xf373X3ge5+Z5J7kvxtd/9KkieTfGhZdjjJY6unBGBj2/F39J9I8vGqOpmz9+wf3oZzAHCZ1ty6+a7u/nKSLy/vX0xy61Z8LwDr+WUswHBCDzCc0AMMJ/QAwwk9wHBCDzCc0AMMJ/QAwwk9wHBCDzCc0AMMJ/QAwwk9wHBCDzCc0AMMJ/QAwwk9wHBCDzCc0AMMJ/QAwwk9wHBCDzCc0AMMJ/QAwwk9wHBCDzCc0AMMJ/QAwwk9wHBCDzCc0AMMJ/QAwwk9wHBCDzDcxqGvqpuq6smqeq6qnq2qjy37b6iqJ6rqheX1+q0bF4ArteaK/q0kv9PdtyS5LclHq+qWJPcnOd7dB5McX7YB2CEbh767T3f3V5f3/5nk+ST7k9yV5Niy7FiSu9cOCcDmtuQefVW9M8m7kzyVZG93n14+ejXJ3oscc6SqTlTVibffPLMVYwBwAatDX1U/mOQvkvxWd//HuZ91dyfpCx3X3Ue7+1B3H9q159q1YwBwEatCX1VX5WzkP9/dX1p2v1ZV+5bP9yV5fd2IAKyx5q9uKsnDSZ7v7j8856PHkxxe3h9O8tjm4wGw1u4Vx96e5FeTfKOqvr7s+/0kDyZ5pKruS/JSkg+vGxGANTYOfXf/fZK6yMd3bPq9AGwtv4wFGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhhB5gOKEHGE7oAYYTeoDhtiX0VXVnVf1zVZ2sqvu34xwAXJ4tD31V7Uryx0nel+SWJPdW1S1bfR4ALs92XNHfmuRkd7/Y3d9J8udJ7tqG8wBwGXZvw3fuT/LyOdunkvzs+Yuq6kiSI8vmf7/00Aef2YZZrkg9tNMTAFyRH7ucRdsR+svS3UeTHE2SqjrR3Yd2ahaAybbj1s0rSW46Z/vAsg+AHbAdof9KkoNVdXNVXZ3kniSPb8N5ALgMW37rprvfqqpfT/I3SXYl+Vx3P3uJw45u9RwAnFXdvdMzALCN/DIWYDihBxhux0PvcQkA22tH79Evj0v4lyS/lLM/rPpKknu7+7kdGwpgmJ2+ove4BIBtttOhv9DjEvbv0CwAI+106AHYZjsdeo9LANhmOx16j0sA2GY79vTKZOPHJQBwBTwCAWC4nb51A8A2E3qA4YQeYDihBxhO6AGGE3qA4YQeYLj/A03O+xlXWZxhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_criterion(max_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

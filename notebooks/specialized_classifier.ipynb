{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from termcolor import colored, cprint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "opts = {}\n",
    "\n",
    "# Data settings\n",
    "opts['num_train_samples'] = 50\n",
    "opts['num_test_samples'] = 100\n",
    "opts['vocab_size'] = 1000 # Number of tokens in vocabulary\n",
    "\n",
    "# Model and ML settings\n",
    "opts['hidden_layer_size']=50\n",
    "opts['learning_rate']=1e-4\n",
    "opts['epochs']=100\n",
    "opts['batch_size']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary. Useful for converting words to tokens and vice versa.\n",
    "# Adapted from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.n_words = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2id:\n",
    "            self.word2id[word] = self.n_words\n",
    "            self.id2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "vocab = Vocabulary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [(7, [0, 1]), (7, [0, 1]), (7, [0, 1])] ...\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset mapping x=some number to a set Y=[0,1]\n",
    "\n",
    "def create_number_to_bimodal_data(num_samples, vocab_size, vocab):\n",
    "    data = []  # Each row will contain (x, [y, y', ...])\n",
    "\n",
    "    # Create vocab\n",
    "    for i in range(vocab_size):\n",
    "        vocab.add_word(str(i)) # if not already there\n",
    "    \n",
    "    # Create data\n",
    "    special_token_1 = vocab.word2id[str(0)]\n",
    "    special_token_2 = vocab.word2id[str(1)]\n",
    "    for i in range(num_samples):\n",
    "        x = 7\n",
    "        x_token = vocab.word2id[str(x)]\n",
    "        data.append((x_token, [special_token_1, special_token_2]))    \n",
    "    return data\n",
    "\n",
    "train_data = create_number_to_bimodal_data(opts['num_train_samples'], opts['vocab_size'], vocab)\n",
    "test_data = create_number_to_bimodal_data(opts['num_test_samples'], opts['vocab_size'], vocab)\n",
    "print('data:',train_data[0:3],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sample: tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "\n",
    "def mark_onehot(i, empty_onehot):\n",
    "    empty_onehot.zero_()\n",
    "    empty_onehot.scatter_(1,torch.tensor([[i]]),1) # e.g. [0,0,1,0]\n",
    "\n",
    "onehot = torch.FloatTensor(1, vocab.n_words)\n",
    "def prep_x(x):\n",
    "    mark_onehot(x, onehot) # e.g. [0,0,1,0]\n",
    "    return onehot\n",
    "\n",
    "def prep_y(y):\n",
    "    return torch.LongTensor([y]) # e.g. y=[[2,0]]\n",
    "\n",
    "print('processed sample:', prep_x(train_data[0][0]), prep_y(train_data[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and loss,or loss\n",
    "class AndLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AndLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        target_onehot = torch.FloatTensor(opts['batch_size'], vocab.n_words-1)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, target, 1)   \n",
    "        loss = torch.log1p(prediction) * target_onehot\n",
    "        loss = -1 * torch.sum(loss)/opts['batch_size']  #Average loss across samples\n",
    "        return loss\n",
    "\n",
    "class OrLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        target_onehot = torch.FloatTensor(opts['batch_size'], vocab.n_words-1)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, target, 1)   \n",
    "        loss = prediction * target_onehot\n",
    "        loss = -1 * torch.log1p(torch.sum(loss))  #Per sample loss\n",
    "#         print(loss)\n",
    "        loss = loss/opts['batch_size']  #Average loss across samples\n",
    "        return loss\n",
    "    \n",
    "class MaxLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        target_onehot = torch.FloatTensor(opts['batch_size'], vocab.n_words-1)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, target, 1)   \n",
    "        loss = prediction * target_onehot\n",
    "        loss = -1 * torch.log1p(torch.max(loss))  #Per sample loss\n",
    "        loss = loss/opts['batch_size']  #Average loss across samples\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "and_criterion = AndLoss()\n",
    "or_criterion = OrLoss()\n",
    "max_criterion = MaxLoss()\n",
    "\n",
    "def official_criterion(prediction, target):\n",
    "    squeezed_target = target.squeeze()\n",
    "    nll = torch.nn.NLLLoss()\n",
    "    return nll(torch.log1p(prediction), squeezed_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show they match when only 1 y\n",
    "# prediction = Variable(torch.rand(opts['batch_size'], vocab.n_words).float())\n",
    "# print('pred '+str(prediction))\n",
    "# target = Variable(torch.FloatTensor(opts['batch_size'],3).uniform_(0, vocab.n_words).long())\n",
    "# print('target '+str(target))\n",
    "\n",
    "# and_loss = and_criterion(prediction, target)\n",
    "# print('and loss '+str(and_loss))\n",
    "# or_loss = or_criterion(prediction, target)\n",
    "# print('or loss '+str(or_loss))\n",
    "# # official_loss = official_criterion(prediction,target)\n",
    "# # print('loss '+str(official_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple neural net\n",
    "def get_new_model():\n",
    "    model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(vocab.n_words, opts['hidden_layer_size']),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(opts['hidden_layer_size'], 2),\n",
    "    torch.nn.Softmax(), # TODO double check need for softmax\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define training\n",
    "def train(data, model, criterion, opts, verbose=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=opts['learning_rate'])\n",
    "    running_loss=0.0\n",
    "    for epoch in range(opts['epochs']):\n",
    "        for (x,y) in data:\n",
    "            x = prep_x(x)\n",
    "            y = prep_y(y)\n",
    "            # Forward pass: compute predicted y\n",
    "            y_pred = model(x) \n",
    "            loss = criterion(y_pred, y)\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            # For printing stats\n",
    "            running_loss += loss.item()\n",
    "        if verbose: print('[Epoch %d] loss: %.3f' % (epoch, running_loss))\n",
    "        running_loss = 0.0\n",
    "        \n",
    "# Define testing\n",
    "def test(x, model, vocab):\n",
    "    output = model(x)\n",
    "    prediction_index = torch.argmax(output)\n",
    "#     prediction = vocab.index2word[prediction_index.item()]\n",
    "    return int(prediction_index.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontColors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_criterion(criterion):\n",
    "    print(\"Training...\")\n",
    "    train(train_data, model, criterion, opts, verbose=False)\n",
    "\n",
    "\n",
    "    print(\"Performance on Train set\")\n",
    "    font_color = FontColors.ENDC\n",
    "    num_correct = 0\n",
    "    for (x,y) in train_data: # Train data\n",
    "        prediction = test(prep_x(x), model, vocab)\n",
    "        is_correct = int(prediction) in y\n",
    "        if is_correct:\n",
    "            num_correct = num_correct + 1\n",
    "            font_color = FontColors.OKGREEN\n",
    "        print(font_color, x, y, '-->', prediction, is_correct, FontColors.ENDC)\n",
    "        font_color = FontColors.ENDC\n",
    "    print(num_correct/opts['num_train_samples'])\n",
    "\n",
    "\n",
    "    print(\"Performance on Test set\")\n",
    "    font_color = FontColors.ENDC\n",
    "    num_correct = 0\n",
    "    predictions = []\n",
    "    for (x,y) in test_data: # Train data\n",
    "        prediction = test(prep_x(x), model, vocab)\n",
    "        predictions.append(vocab.id2word[prediction])\n",
    "        is_correct = int(prediction) in y\n",
    "        if is_correct:\n",
    "            num_correct = num_correct + 1\n",
    "            font_color = FontColors.OKGREEN\n",
    "        print(font_color, x, y, '-->', prediction, is_correct, FontColors.ENDC)\n",
    "        font_color = FontColors.ENDC\n",
    "    print(num_correct/opts['num_test_samples'])\n",
    "    \n",
    "    plt.hist(predictions)\n",
    "    plt.axis([0,1,0,opts['num_test_samples']])\n",
    "#     plt.xlabel('Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (999) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0bad24f0dbd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mand_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-e0985478b505>\u001b[0m in \u001b[0;36mevaluate_criterion\u001b[0;34m(criterion)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-7a76d74cfdc5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, model, criterion, opts, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Forward pass: compute predicted y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Backward pass: compute gradient of the loss with respect to model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/specialize-project/venvs/c-pcnn-gated/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-f661f170e184>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prediction, target)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtarget_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtarget_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_onehot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#Average loss across samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (999) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "evaluate_criterion(and_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Performance on Train set\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "1.0\n",
      "Performance on Test set\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "\u001b[92m 0 [0, 1] --> 1 True \u001b[0m\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_criterion(or_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pkalluri/env/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Train set\n",
      "\u001b[92m 117 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 129 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 770 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 363 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 33 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 435 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 734 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 520 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 633 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 248 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 614 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 456 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 738 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 795 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 98 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 767 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 24 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 514 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 702 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 602 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 839 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 997 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 476 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 848 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 577 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 622 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 180 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 142 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 572 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 832 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 865 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 357 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 77 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 493 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 345 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 121 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 649 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 811 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 268 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 706 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 36 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 618 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 146 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 96 [0, 1] --> 0 True \u001b[0m\n",
      "1.0\n",
      "Performance on Test set\n",
      "\u001b[92m 977 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 596 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 179 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 74 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 821 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 316 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 227 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 561 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 57 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 647 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 992 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 636 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 566 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 594 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 688 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 153 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 845 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 451 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 621 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 929 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 392 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 993 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 587 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 59 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 460 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 872 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 966 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 220 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 487 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 454 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 214 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 390 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 64 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 499 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 497 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 306 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 758 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 936 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 797 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 206 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 644 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 625 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 485 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 169 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 612 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 652 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 256 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 827 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 112 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 374 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 269 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 411 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 182 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 694 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 604 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 829 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 964 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 455 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 288 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 94 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 40 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 983 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 999 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 915 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 642 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 236 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 165 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 346 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 379 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 855 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 174 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 971 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 826 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 972 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 783 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 389 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 501 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 474 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 323 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 859 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 746 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 429 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 381 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 266 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 208 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 567 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 693 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 513 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 907 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 307 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 380 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 710 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 187 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 309 [0, 1] --> 0 True \u001b[0m\n",
      "\u001b[92m 803 [0, 1] --> 0 True \u001b[0m\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_criterion(max_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
